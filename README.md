# Projeto de Pipeline ETL com Airflow

Este projeto demonstra a construÃ§Ã£o de um pipeline ETL (Extract, Transform, Load) para integrar dados de diferentes fontes em um Data Warehouse centralizado. A orquestraÃ§Ã£o do fluxo de trabalho Ã© realizada utilizando Apache Airflow, e todo o ambiente Ã© gerenciado com Docker Compose para garantir a reprodutibilidade.

## âš™ï¸ Tecnologias Utilizadas
Apache Airflow: Orquestrador de workflows para agendamento, monitoramento e execuÃ§Ã£o do pipeline ETL.

Docker e Docker Compose: Usados para criar e gerenciar um ambiente de desenvolvimento isolado, com todos os serviÃ§os necessÃ¡rios (bancos de dados, Airflow, etc.).

PostgreSQL: Utilizado tanto como banco de dados de origem (simulando o sistema transacional BanVic) quanto como Data Warehouse (DWH) de destino.

Python: Linguagem de programaÃ§Ã£o para o desenvolvimento das tarefas de extraÃ§Ã£o e carregamento de dados.

Pandas: Biblioteca para manipulaÃ§Ã£o e processamento de dados em Python.

SQLAlchemy: Toolkit SQL para facilitar a conexÃ£o e a interaÃ§Ã£o com os bancos de dados.

## ğŸ“ Estrutura do Projeto
A estrutura do projeto Ã© organizada da seguinte forma:

â””â”€â”€ docker-compose.yml

â”œâ”€â”€ dags/

â”‚   â””â”€â”€ banvic_pipeline.py

â”œâ”€â”€ data/

â”œâ”€â”€ dbdata/

â”œâ”€â”€ dwhdata/

â”œâ”€â”€ airflow_metadata/


dags/: ContÃ©m o cÃ³digo do pipeline ETL (banvic_pipeline.py) que Ã© lido pelo Airflow.

data/: DiretÃ³rio para armazenar os arquivos CSV extraÃ­dos temporariamente. 

dbdata/: Volume persistente para os dados do banco de origem.

dwhdata/: Volume persistente para os dados do Data Warehouse.

airflow_metadata/: Volume persistente para os metadados do Airflow.

docker-compose.yml: Arquivo de configuraÃ§Ã£o que define e interconecta todos os serviÃ§os (contÃªineres) do projeto.

As pastas data, dbdata, dwhdata e airflow_metadata serÃ£o criadas automÃ¡ticamente apÃ³s a execuÃ§Ã£o.

## ğŸš€ Como Executar o Projeto
Para colocar o projeto em funcionamento, vocÃª sÃ³ precisa ter o Docker e o Docker Compose instalados na sua mÃ¡quina.

Clone o RepositÃ³rio:

```Bash
git clone https://github.com/joaocastro26/DesafioIndicium.git
cd DesafioIndicium
```
Inicie os ContÃªineres:
No diretÃ³rio raiz do projeto, execute o comando abaixo para construir as imagens e iniciar todos os serviÃ§os em segundo plano.

```Bash

docker-compose up -d
```
Aguarde alguns minutos. Na primeira execuÃ§Ã£o, o Docker irÃ¡ baixar as imagens necessÃ¡rias e os serviÃ§os de inicializaÃ§Ã£o do Airflow configurarÃ£o o ambiente, incluindo a criaÃ§Ã£o de um usuÃ¡rio administrador.

Acesse a Interface do Airflow:
ApÃ³s os contÃªineres estarem em execuÃ§Ã£o, abra seu navegador e acesse:
```Bash
http://localhost:8080
```
FaÃ§a login com as credenciais padrÃ£o:

```Bash
Login: airflow

Senha: airflow
```
Execute o Pipeline:
Na interface do Airflow, localize a DAG chamada banvic_pipeline. VocÃª pode ativÃ¡-la e disparar uma execuÃ§Ã£o manualmente clicando no botÃ£o de Play. O Airflow irÃ¡ entÃ£o executar as tarefas de extraÃ§Ã£o e carregamento, movendo os dados das fontes para o Data Warehouse.

## ğŸ¯ VisÃ£o Geral do Pipeline
O pipeline banvic_pipeline.py Ã© um fluxo de trabalho ETL com trÃªs etapas principais:

ExtraÃ§Ã£o de Dados (extract):

Uma tarefa extrai dados do arquivo transacoes.csv.

Outra tarefa se conecta ao banco de dados PostgreSQL de origem (db) e extrai dados de mÃºltiplas tabelas (agencias, clientes, contas, etc.).

Ambas as extraÃ§Ãµes rodam em paralelo e salvam os dados em arquivos CSV temporÃ¡rios.

SincronizaÃ§Ã£o (sync):

Um operador de sincronizaÃ§Ã£o (EmptyOperator) garante que a prÃ³xima etapa sÃ³ comece se todas as tarefas de extraÃ§Ã£o tiverem sido concluÃ­das com sucesso.

Carregamento de Dados (load):

A etapa final carrega os dados dos arquivos CSV extraÃ­dos para as tabelas correspondentes no Data Warehouse (dwh_postgres). A lÃ³gica de carregamento utiliza a funÃ§Ã£o to_sql do pandas, que gerencia a criaÃ§Ã£o das tabelas no destino, garantindo a idempotÃªncia do processo.
